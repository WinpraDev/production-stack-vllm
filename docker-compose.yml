version: '3.8'

services:
  # vLLM Engine - The actual model serving component
  vllm-engine:
    image: vllm/vllm-openai:latest
    container_name: vllm-engine
    runtime: nvidia
    environment:
      # Model Configuration
      - MODEL_NAME=${MODEL_NAME:-facebook/opt-125m}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HOME=/root/.cache/huggingface
      
      # vLLM Configuration
      - VLLM_API_KEY=${VLLM_API_KEY:-your-vllm-api-key}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-2048}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.95}
      - TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
      - DTYPE=${DTYPE:-auto}
      - TRUST_REMOTE_CODE=${TRUST_REMOTE_CODE:-false}
      
      # CUDA Configuration
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - model_data:/workspace/models
    ports:
      - "7100:8000"  # Direct vLLM access
    command:
      - --host=0.0.0.0
      - --port=8000
      - --model=${MODEL_NAME:-facebook/opt-125m}
      - --api-key=${VLLM_API_KEY:-your-vllm-api-key}
      - --tensor-parallel-size=${TENSOR_PARALLEL_SIZE:-1}
      - --max-model-len=${MAX_MODEL_LEN:-2048}
      - --dtype=${DTYPE:-auto}
      - --gpu-memory-utilization=${GPU_MEMORY_UTILIZATION:-0.95}
      - --enable-prefix-caching
      - --enable-chunked-prefill
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 32G
    ipc: host
    shm_size: '16gb'
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    restart: unless-stopped
    networks:
      - vllm-network

  # Router - Load balancer and request router (commented out for simplicity)
  # vllm-router:
  #   image: lmcache/lmstack-router:latest
  #   container_name: vllm-router
  #   environment:
  #     - VLLM_API_KEY=${VLLM_API_KEY:-your-vllm-api-key}
  #   ports:
  #     - "7200:8000"  # Router port (if enabled)
  #   command:
  #     - --service-discovery=static
  #     - --static-backends=vllm-engine:8000
  #     - --static-models=facebook/opt-125m
  #     - --routing-logic=roundrobin
  #     - --host=0.0.0.0
  #     - --port=8000
  #   depends_on:
  #     - vllm-engine
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #   restart: unless-stopped
  #   networks:
  #     - vllm-network

  # Prometheus for metrics collection  
  prometheus:
    image: prom/prometheus:latest
    container_name: vllm-prometheus
    volumes:
      - prometheus_data:/prometheus
    ports:
      - "7101:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    user: "0:0"
    entrypoint: |
      sh -c '
      mkdir -p /etc/prometheus
      cat > /etc/prometheus/prometheus.yml << EOF
      global:
        scrape_interval: 15s
        evaluation_interval: 15s

      scrape_configs:
        - job_name: "prometheus"
          static_configs:
            - targets: ["localhost:9090"]

        - job_name: "vllm-engine"
          static_configs:
            - targets: ["vllm-engine:8000"]
          metrics_path: /metrics
          scrape_interval: 10s
          scrape_timeout: 5s
      EOF
      exec /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.enable-lifecycle
      '
    depends_on:
      - vllm-engine
    networks:
      - vllm-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: vllm-grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "7102:3000"
    depends_on:
      - prometheus
    networks:
      - vllm-network

networks:
  vllm-network:
    driver: bridge

volumes:
  huggingface_cache:
    driver: local
  model_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local